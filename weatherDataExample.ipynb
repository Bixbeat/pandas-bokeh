{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandasql import sqldf\n",
    "import folium\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = r'D:\\git\\pandas-bokeh\\data\\KNMI_20161227.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_weather_data(file_name,station_numbers):\n",
    "    \n",
    "    weather_header_list = []\n",
    "    weather_dict = {}\n",
    "    spatial_header_list = []\n",
    "    spatial_dict = {}\n",
    "    \n",
    "    def clear_whitespace(element, characters_to_remove):\n",
    "        characters_in_element = [character for character in list(element) if character not in characters_to_remove]\n",
    "        cleaned_element = ''.join(characters_in_element)\n",
    "        return cleaned_element\n",
    "    \n",
    "    def process_headers(header_line, header_list, header_dict, separator, unique_id = 'False'):\n",
    "        selected_header_line = header_line\n",
    "        split_header_list = selected_header_line.split(separator)\n",
    "        #Determine headers in the main file (operations in order: remove hashtag, append main body, remove trailing line break)\n",
    "        uncleaned_header_list = [split_header_list[0][2:]]\\\n",
    "                            + split_header_list[1:-1] \\\n",
    "                            + [split_header_list[-1][:-1]]\n",
    "                \n",
    "        for element in uncleaned_header_list:\n",
    "            cleaned_header = clear_whitespace(element, (' '))\n",
    "            #Ensure that only valid headers are added (empty = not appended)\n",
    "            if len(cleaned_header) > 0:\n",
    "                header_list.append(cleaned_header)\n",
    "                header_dict[cleaned_header] = []\n",
    "        if unique_id:\n",
    "            header_list.append('ID')\n",
    "            header_dict['ID'] = []\n",
    "            \n",
    "    \n",
    "    with open(file_name) as data:\n",
    "        loaded_data = data.readlines()\n",
    "        \n",
    "        '''Process weather data header'''\n",
    "        process_headers(loaded_data[97], weather_header_list, weather_dict, ',', unique_id = True)\n",
    "        \n",
    "        '''Process main weather data'''\n",
    "        uncleaned_weather_data = [line for line in loaded_data[100:] if line[2:5] in station_numbers]#stationNumber]\n",
    "        weather_data = [line.split(',') for line in uncleaned_weather_data[:]]        \n",
    "        \n",
    "        for linenumber, lines in enumerate(weather_data):\n",
    "            for element_number, elements in enumerate(lines):\n",
    "                cleaned_element = clear_whitespace(elements,(' ', '\\n'))\n",
    "                #Error handling required to prevent unexpected EOF while parsing - no known alternatives         \n",
    "                try:\n",
    "                    evaluated_value = eval(cleaned_element)\n",
    "                    weather_dict[weather_header_list[element_number]].append(evaluated_value)                    \n",
    "                except:\n",
    "                    weather_dict[weather_header_list[element_number]].append(cleaned_element)\n",
    "            weather_dict['ID'].append(linenumber)\n",
    "        \n",
    "        '''Process spatail data headers'''   \n",
    "        process_headers(loaded_data[4], spatial_header_list, spatial_dict, ' ')\n",
    "        \n",
    "        '''Process main spatial data'''\n",
    "        uncleaned_spatial_data = loaded_data[5:55]\n",
    "        #Splitting spatial data lines\n",
    "        split_spatial_data = [line.split() for line in uncleaned_spatial_data[:]]    \n",
    "        spatial_data = list(map(lambda values: values[1:], split_spatial_data))\n",
    "        #Remove trailing colon after first element\n",
    "        for lines in spatial_data:\n",
    "            lines[0] = lines[0][:-1]\n",
    "            while len(lines) > len(spatial_header_list):\n",
    "                lines[len(spatial_header_list)-1] = str(lines[len(spatial_header_list)-1]) + ' ' + str(lines[len(spatial_header_list)])\n",
    "                del lines[len(spatial_header_list)]\n",
    "            for element_number, element in enumerate(lines):\n",
    "                try:\n",
    "                    spatial_dict[spatial_header_list[element_number]].append(eval(element))\n",
    "                except:\n",
    "                    spatial_dict[spatial_header_list[element_number]].append(element)\n",
    "        \n",
    "        '''Combine header files'''\n",
    "        headers = weather_header_list[:] + spatial_header_list[:]\n",
    "            \n",
    "    return weather_dict, spatial_dict, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading time is approx 9 seconds for two stations selecting the full list of records but rises quickly with more stations\n",
    "#It does however significantly reduce the overall loading time as pandas does not handle large dataframes efficiently\n",
    "#It is advised to not load more than a couple of stations at once, e.g. by looping\n",
    "data = load_weather_data(file_name,('225'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perform_SQL = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Joining the two datasets together\n",
    "def join_dataframes(weather_dict, spatial_dict):\n",
    "    weather_DF = pd.DataFrame(weather_dict)[1:].apply(pd.to_numeric)\n",
    "    spatial_DF = pd.DataFrame(spatial_dict)\n",
    "\n",
    "    #Runtime is about 5mins\n",
    "    sql = \"\"\"\n",
    "            SELECT * FROM weather_DF\n",
    "            JOIN spatial_DF ON spatial_DF.stn = weather_DF.stn;\n",
    "          \"\"\"\n",
    "\n",
    "    weather_and_spatial_data_DF = perform_SQL(sql)\n",
    "    return weather_and_spatial_data_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unique_stations(file_name):\n",
    "    with open(file_name) as data:\n",
    "        loaded_data = data.readlines()\n",
    "        unique_numbers = sorted(list(set(list([line[2:5] for line in loaded_data[100:]]))))\n",
    "    return unique_numbers\n",
    "unique_station_numbers = get_unique_stations(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PandaSQLException",
     "evalue": "(sqlite3.OperationalError) no such table: weather_DF [SQL: 'ALTER table \"weather_DF\" ADD COLUMN id;']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPandaSQLException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d052b3446558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msql\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"ALTER table \"weather_DF\" ADD COLUMN id;\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mperform_SQL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;31m#Get weather data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1ae19d4165c9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(q)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperform_SQL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msqldf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Alex\\Anaconda3\\envs\\py27\\lib\\site-packages\\pandasql\\sqldf.pyc\u001b[0m in \u001b[0;36msqldf\u001b[0;34m(query, env, db_uri)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0msqldf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select avg(x) from df;\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mPandaSQL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb_uri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Alex\\Anaconda3\\envs\\py27\\lib\\site-packages\\pandasql\\sqldf.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, query, env)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mDatabaseError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mPandaSQLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mResourceClosedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[1;31m# query returns nothing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPandaSQLException\u001b[0m: (sqlite3.OperationalError) no such table: weather_DF [SQL: 'ALTER table \"weather_DF\" ADD COLUMN id;']"
     ]
    }
   ],
   "source": [
    "#Runtime for an avg selection takes about 5min per marker\n",
    "#Plots graphs into folium markers, then adds markers to map & loads map inline\n",
    "#evening_map = folium.Map(location=[52.092560, 5.109378],zoom_start=13)\n",
    "\n",
    "plot_map = folium.Map(location=[52.092560, 5.109378],zoom_start=10)\n",
    "\n",
    "for number in unique_station_numbers:\n",
    "    loaded_data = load_weather_data(file_name, number)\n",
    "    weather_DF = pd.DataFrame(loaded_data[0])[1:].apply(pd.to_numeric)\n",
    "    spatial_DF = pd.DataFrame(loaded_data[1])    \n",
    "    \n",
    "    #Find biggest difference in temperature in two days\n",
    "    sql = \"\"\"\n",
    "            SELECT day1.yyyymmdd, day1.id, day1.temp, IFNULL(day1.temp - day2.temp, 0) AS difference\n",
    "            FROM weather_DF as day1\n",
    "                JOIN weather_DF day2 ON day1.id = day2.id AND day2.id - day1.id\n",
    "          \"\"\"\n",
    "    \n",
    "    #Get weather data\n",
    "    sql = \"\"\"\n",
    "            SELECT STN, MAX(FG) FROM weather_DF\n",
    "            WHERE STN = {unique_station_numbers}\n",
    "          \"\"\".format(unique_station_numbers = number)\n",
    "    max_wind_speed_list = perform_SQL(sql).to_csv(None, header=False, index=False).split('\\n')[:-1]\n",
    "    max_wind_speed_list = max_wind_speed_list[0].split(',')\n",
    "    \n",
    "        #Get weather data\n",
    "    sql = \"\"\"\n",
    "            SELECT MAX(\n",
    "                \n",
    "                ;)\n",
    "            FROM weather_DF\n",
    "            WHERE STN = {unique_station_numbers}\n",
    "          \"\"\".format(unique_station_numbers = number)\n",
    "    biggest_tempdif = perform_SQL(sql).to_csv(None, header=False, index=False).split('\\n')[:-1]\n",
    "    biggest_tempdif = max_wind_speed_list[0].split(',')\n",
    "    \n",
    "    #Get spatial data\n",
    "    sql = \"\"\"\n",
    "            SELECT \"LAT(north)\", \"LON(east)\" FROM spatial_DF\n",
    "            WHERE STN = {unique_station_numbers}\n",
    "          \"\"\".format(unique_station_numbers = number)\n",
    "    spatial_data_list = perform_SQL(sql).to_csv(None, header=False, index=False).split('\\n')[:-1]\n",
    "    spatial_data_list = spatial_data_list[0].split(',')\n",
    "\n",
    "    folium.Marker([spatial_data_list[0], spatial_data_list[1]],\\\n",
    "        popup=folium.Popup(folium.element.IFrame(html=\\\n",
    "        '''\n",
    "            Station: {stn} <br>\n",
    "            Maximum wind speed: {fg}\n",
    "        '''.format(stn = max_wind_speed_list[0], fg = max_wind_speed_list[1]),\\\n",
    "        width=200, height=200),\\\n",
    "        max_width=200)).add_to(plot_map)\n",
    "\n",
    "    #folium.Marker([spatial_data_list[2], spatial_data_list[1]],\\\n",
    "    #    popup=folium.Popup(\\\n",
    "    #    folium.element.IFrame(html=\"hi\",\\\n",
    "    #    width=200, height=200),\\\n",
    "    #    max_width=200))\\\n",
    "    #    .add_to(plot_map), axis =1 \n",
    "    \n",
    "plot_map.save('plot_map.html')\n",
    "plot_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folium.Marker([spatial_data_list[2], spatial_data_list[1]],\\\n",
    "    popup=folium.Popup(\\\n",
    "    folium.element.IFrame(html=\\\n",
    "    '''\n",
    "        Station: {stn} <br>\n",
    "        Maximum wind speed: {fg}\n",
    "    '''.format(stn = max_wind_speed_list[0], fg = max_wind_speed_list[1]),\n",
    "    width=200, height=200),\\\n",
    "    max_width=200))\\\n",
    "    .add_to(plot_map), axis =1 \n",
    "    \n",
    "folium.Marker([spatial_data_list[1], spatial_data_list[0]],\\\n",
    "          popup=folium.Popup(folium.element.IFrame(html=\"hi\",\\\n",
    "width=200, height=200),\\\n",
    "max_width=200)).add_to(plot_map)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

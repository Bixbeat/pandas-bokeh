{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandasql import sqldf\n",
    "import folium\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getUniqueStations(fileName):\n",
    "    with open(fileName) as data:\n",
    "        loadedData = data.readlines()\n",
    "        uniqueNumbers = sorted(list(set(list([stn[2:5] for stn in loadedData[100:]]))))\n",
    "    return uniqueNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['209', '210', '215', '225', '235', '240', '242', '248', '249', '251', '257', '258', '260', '265', '267', '269', '270', '273', '275', '277', '278', '279', '280', '283', '285', '286', '290', '308', '310', '311', '312', '313', '315', '316', '319', '323', '324', '330', '331', '340', '343', '344', '348', '350', '356', '370', '375', '377', '380', '391']\n"
     ]
    }
   ],
   "source": [
    "uniqueStationNumbers = getUniqueStations(r'D:\\git\\pandas-bokeh\\data\\KNMI_20161227.txt')\n",
    "print uniqueStationNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadWeatherData(fileName,stationNumbers):\n",
    "    \n",
    "    weatherHeaderList = []\n",
    "    weatherDict = {}\n",
    "    spatialHeaderList = []\n",
    "    spatialDict = {}\n",
    "    \n",
    "    def clearWhitespace(element, charactersToRemove):\n",
    "        charactersInElement = [character for character in list(element) if character not in charactersToRemove]\n",
    "        cleanedElement = ''.join(charactersInElement)\n",
    "        return cleanedElement\n",
    "    \n",
    "    def cleanHeader(headerLine, headerList, headerDict, separator):\n",
    "        selectedHeaderLine = headerLine\n",
    "        splitHeaderList = selectedHeaderLine.split(separator)\n",
    "        #Determine headers in the main file (operations in order: remove hashtag, append main body, remove trailing line break)\n",
    "        uncleanedHeaderList = [splitHeaderList[0][2:]]\\\n",
    "                            + splitHeaderList[1:-1] \\\n",
    "                            + [splitHeaderList[-1][:-1]]\n",
    "        for element in uncleanedHeaderList:\n",
    "            cleanedHeader = clearWhitespace(element, (' '))\n",
    "            #Ensure that only valid headers are added (empty = not appended)\n",
    "            if len(cleanedHeader) > 0:\n",
    "                headerList.append(cleanedHeader)\n",
    "                headerDict[cleanedHeader] = []\n",
    "    \n",
    "    with open(fileName) as data:\n",
    "        loadedData = data.readlines()\n",
    "        \n",
    "        '''Process weather data header'''\n",
    "        cleanHeader(loadedData[97], weatherHeaderList, weatherDict, ',')     \n",
    "        \n",
    "        '''Process main weather data'''\n",
    "        uncleanedWeatherData = [line for line in loadedData[100:] if line[2:5] in stationNumbers]#stationNumber]\n",
    "        weatherData = [line.split(',') for line in uncleanedWeatherData[:]]        \n",
    "        \n",
    "        for lines in weatherData:\n",
    "            for elementNumber, elements in enumerate(lines):\n",
    "                cleanedElement = clearWhitespace(elements,(' ', '\\n'))\n",
    "                #Error handling required to prevent unexpected EOF while parsing - no known alternatives         \n",
    "                try:\n",
    "                    evaluatedValue = eval(cleanedElement)\n",
    "                    weatherDict[weatherHeaderList[elementNumber]].append(evaluatedValue)                    \n",
    "                except:\n",
    "                    weatherDict[weatherHeaderList[elementNumber]].append(cleanedElement)\n",
    "        \n",
    "        '''Process spatail data headers'''   \n",
    "        cleanHeader(loadedData[4], spatialHeaderList, spatialDict, ' ')\n",
    "        \n",
    "        '''Process main spatial data'''\n",
    "        uncleanedSpatialData = loadedData[5:54]\n",
    "        #Splitting spatial data lines\n",
    "        splitSpatialData = [line.split() for line in uncleanedSpatialData[:]]    \n",
    "        spatialData = list(map(lambda values: values[1:], splitSpatialData))\n",
    "        #Remove trailing colon after first element\n",
    "        for lines in spatialData:\n",
    "            lines[0] = lines[0][:-1]\n",
    "            while len(lines) > len(spatialHeaderList):\n",
    "                lines[len(spatialHeaderList)-1] = str(lines[len(spatialHeaderList)-1]) + ' ' + str(lines[len(spatialHeaderList)])\n",
    "                del lines[len(spatialHeaderList)]\n",
    "            for elementNumber, element in enumerate(lines):\n",
    "                try:\n",
    "                    spatialDict[spatialHeaderList[elementNumber]].append(eval(element))\n",
    "                except:\n",
    "                    spatialDict[spatialHeaderList[elementNumber]].append(element)\n",
    "        \n",
    "        '''Combine header files'''\n",
    "        headers = weatherHeaderList[:] + spatialHeaderList[:]\n",
    "            \n",
    "    return weatherDict, spatialDict, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading time is approx 9 seconds for two stations selecting the full list of records but rises quickly with more stations\n",
    "#It does however significantly reduce the overall loading time as pandas does not handle large dataframes efficiently\n",
    "#It is advised to not load more than a couple of stations at once, e.g. by looping\n",
    "data = loadWeatherData(r'D:\\git\\pandas-bokeh\\data\\KNMI_20161227.txt',('209'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Joining the two datasets together\n",
    "weatherDF = pd.DataFrame(data[0])[1:].apply(pd.to_numeric)\n",
    "spatialDF = pd.DataFrame(data[1])\n",
    "\n",
    "#Runtime is about 5mins\n",
    "performSQL = lambda q: sqldf(q, globals())\n",
    "\n",
    "sql = \"\"\"\n",
    "        SELECT * FROM weatherDF\n",
    "        JOIN spatialDF ON spatialDF.stn = weatherDF.stn;\n",
    "      \"\"\"\n",
    "\n",
    "weatherAndSpatialDataDF = performSQL(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['209']\n"
     ]
    }
   ],
   "source": [
    "#Select unique station names\n",
    "sql = \"\"\"\n",
    "        SELECT DISTINCT(STN) FROM weatherAndSpatialDataDF\n",
    "      \"\"\"\n",
    "#List index -1 is to filter a trailing empty record that is returned with this syntax\n",
    "uniqueStationNumbers = performSQL(sql).to_csv(None, header=False, index=False).split('\\n')[:-1]\n",
    "print uniqueStationNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['73.1510370502']\n"
     ]
    }
   ],
   "source": [
    "#Runtime for an avg selection takes about 5min per marker\n",
    "#Plots graphs into folium markers, then adds markers to map & loads map inline\n",
    "#evening_map = folium.Map(location=[52.092560, 5.109378],zoom_start=13)\n",
    "\n",
    "for number in uniqueStationNumbers:\n",
    "    sql = \"\"\"\n",
    "        SELECT avg(FG) FROM weatherAndSpatialDataDF\n",
    "        WHERE STN = {stationNumber}\n",
    "      \"\"\".format(stationNumber = number)\n",
    "    maxWindSpeedList = performSQL(sql).to_csv(None, header=False, index=False).split('\\n')[:-1]\n",
    "    print maxWindSpeedList\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT \"LON(east)\", \"LAT(north)\", \"NAME\" FROM weatherAndSpatialDataDF\n",
    "        WHERE STN = {stationNumber}\n",
    "        GROUP BY NAME\n",
    "      \"\"\".format(stationNumber = number)\n",
    "    stationLocationAndName = performSQL(sql).to_csv(None, header=False, index=False).split('\\n')[:-1]\n",
    "    stationLocationAndName = stationLocationAndName[0].split(',')\n",
    "    \n",
    "    #Specify path to load figures from\n",
    "    #url = r\"http://localhost:8888/files/UtrechtTraffic/utrecht/git/plots/{}.png\".format(code)\n",
    "    #graph ='<img src=\"{}\">'.format(url)\n",
    "\n",
    "    #http://gis.stackexchange.com/questions/185897/how-can-i-include-html-in-a-folium-marker-popup\n",
    "    #\n",
    "    #marker.apply(lambda row: folium.Marker([row['latitude'], row['longitude']],\\\n",
    "    #                        popup=folium.Popup(folium.element.IFrame(html=graph,\n",
    "    #                        width=550, height=500),\\\n",
    "    #                        max_width=550))\\\n",
    "    #                        .add_to(evening_map), axis =1) \n",
    "#evening_map.save('evening.html')\n",
    "#evening_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
